{
  "AttachmentsContent": [],
  "Content": "{\n  \"description\" : \"## Id\\nAWSResilienceHub-SimulateS3ObjectsAccidentalDeleteTest_2020-04-01\\n\\n## Intent\\nTest the case where all versions of files in a subfolder in the bucket were deleted and restored from a backup bucket\\n\\n## Type\\nTEST\\n\\n## Risk\\nHigh\\n\\n## Requirements\\n  * S3 bucket on which the mock subfolder to be cleaned, will be created on\\n  * Objects in the test subfolder on that bucket are constantly requested during test\\n  * Alarm for metric [4xxErrors](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html) setup for the S3 bucket\\n  * S3 bucket that will be used to backup the subfolder after deletion\\n\\n## Permissions required for AutomationAssumeRole\\n  * s3:DeleteObject\\n  * s3:GetObject\\n  * s3:DeleteObjectVersion\\n  * s3:ListBucket\\n  * s3:ListBucketVersions\\n  * s3:ListObjectsV2\\n  * s3:ListObjectVersions\\n  * s3:PutObject\\n  * ssm:StartAutomationExecution\\n  * ssm:GetAutomationExecution\\n  * sns:Publish\\n  * ssm:GetParameters\\n  * cloudwatch:DescribeAlarms\\n  * iam:PassRole\\n\\n##### Permissions required in case KMS encryption is used\\n  * kms:GenerateDataKey\\n  * kms:Decrypt\\n  * kms:Encrypt\\n\\n## Supports Rollback\\nNo. No rollback behavior is needed as injected failure affects mock data which exists only for the test. \\\"onCancel\\\" and \\\"onFailure\\\" will skip to a step which will delete said subfolder if it exists.\\n\\n## Cancellation behavior\\nDelete test subfolder if it exists.\\n\\n## Inputs\\n### (Required) AutomationAssumeRole\\n  * type: String\\n  * description: ARN of the IAM role with permissions listed above\\n\\n### (Required) S3BucketWhereObjectsWillBeDeletedFrom\\n  * type: String\\n  * description: The S3 Bucket Name where objects will be deleted\\n\\n### (Required) S3BucketToRestoreWhereObjectWillBeCopiedTo\\n  * type: String\\n  * description: The S3 Bucket Name where objects will be copied\\n\\n### (Required) S3UserErrorAlarmName\\n  * type: String\\n  * description: Alarm for metric [4xxErrors](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metrics-dimensions.html) setup for the S3 bucket\\n\\n### (Required) SNSTopicARNForManualApproval\\n  * type: String\\n  * description: The ARN of the SNS Topic where a user will receive the notification about the manual approval of the creation of the test subfolder.\\n\\n### (Required) SNSTopicIAMPrincipalForManualApproval\\n  * type: String\\n  * description:\\n    ARN of AWS authenticated principal which is able to either approve or reject the creation of the test subfolder. Can be either an AWS Identity and Access\\n    Management (IAM) user name or IAM user ARN or IAM role ARN or IAM assume role user ARN\\n\\n## Details\\nThe document creates a subfolder in the format of \\\"arh-4xxErrors-test-timestamp\\\" with 10 objects inside. It injects failure by clearing the objects from that subfolder. Those objects should be constantly\\nrequested to trigger the alarm for metric `4xxErrors`.\\n\\n## Steps executed in normal flow\\n  * GenerateSubfolderNameBeforeCreation\\n  * CheckNoSubfolderOnBothBuckets\\n  * AssertNoSubfolderOnBothBuckets\\n  * ApproveSubfolderCreation\\n  * CreateSubfolderWithTestFiles\\n  * AssertAlarmToBeGreenBeforeTest\\n  * BackupCurrentExecution\\n  * InjectFailure\\n  * AssertAlarmToBeRed\\n  * RollbackCurrentExecution\\n  * AssertAlarmToBeGreen\\n  * DeleteSubfolderAfterTest\\n\\n## Outputs\\nNone\",\n  \"schemaVersion\" : \"0.3\",\n  \"assumeRole\" : \"{{ AutomationAssumeRole }}\",\n  \"parameters\" : {\n    \"S3BucketWhereObjectsWillBeDeletedFrom\" : {\n      \"type\" : \"String\",\n      \"description\" : \"(Required) The S3 Bucket Name where objects will be deleted.\"\n    },\n    \"S3BucketToRestoreWhereObjectWillBeCopiedTo\" : {\n      \"type\" : \"String\",\n      \"description\" : \"(Required) The S3 Bucket Name where objects will be copied.\"\n    },\n    \"AutomationAssumeRole\" : {\n      \"type\" : \"String\",\n      \"description\" : \"(Required) The ARN of the role that allows Automation to perform the actions on your behalf.\"\n    },\n    \"S3UserErrorAlarmName\" : {\n      \"type\" : \"String\",\n      \"description\" : \"(Required) Alarm for metric `4xxErrors` setup for the S3 bucket\"\n    },\n    \"SNSTopicARNForManualApproval\" : {\n      \"type\" : \"String\",\n      \"description\" : \"(Required) The ARN of the SNS Topic where a user will receive the notification about the manual approval of the creation of the test subfolder.\"\n    },\n    \"IAMPrincipalForManualApproval\" : {\n      \"type\" : \"String\",\n      \"description\" : \"(Required) ARN of AWS authenticated principal which is able to either approve or reject the creation of the test subfolder. Can be either an AWS Identity and Access Management (IAM) user name or IAM user ARN or IAM role ARN or IAM assume role user ARN\"\n    }\n  },\n  \"mainSteps\" : [ {\n    \"name\" : \"GenerateSubfolderNameBeforeCreation\",\n    \"description\" : \"Get the name of the test subfolder in the format of 'prefix-timestamp'.\",\n    \"action\" : \"aws:executeScript\",\n    \"outputs\" : [ {\n      \"Name\" : \"Subfolder\",\n      \"Selector\" : \"$.Payload.Subfolder\",\n      \"Type\" : \"String\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"generate_test_subfolder_name_before_creation\",\n      \"InputPayload\" : {\n        \"SubfolderPrefix\" : \"arh-4xxErrors-\"\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\ndef generate_test_subfolder_name_before_creation(events, context):\\n    if ('SubfolderPrefix' not in events):\\n        raise KeyError('Requires SubfolderPrefix in events')\\n\\n    subfolder = f\\\"{events['SubfolderPrefix']}-{math.floor(time.time())}\\\"\\n\\n    return {'Subfolder': subfolder}\"\n    }\n  }, {\n    \"name\" : \"CheckNoSubfolderOnBothBuckets\",\n    \"description\" : \"Check if there is no subfolder in the format of 'arh-4xxErrors-test-timestamp' on both source and target buckets.\",\n    \"action\" : \"aws:executeScript\",\n    \"outputs\" : [ {\n      \"Name\" : \"NoSubfolderOnBothBuckets\",\n      \"Selector\" : \"$.Payload.NoSubfolderOnBothBuckets\",\n      \"Type\" : \"Boolean\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"check_no_subfolder_on_both_buckets\",\n      \"InputPayload\" : {\n        \"SourceBucket\" : \"{{S3BucketWhereObjectsWillBeDeletedFrom}}\",\n        \"TargetBucket\" : \"{{S3BucketToRestoreWhereObjectWillBeCopiedTo}}\",\n        \"Subfolder\" : \"{{GenerateSubfolderNameBeforeCreation.Subfolder}}\"\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\ndef check_no_subfolder_on_both_buckets(events, context):\\n    if ('SourceBucket' not in events or 'TargetBucket' not in events\\n            or 'Subfolder' not in events):\\n        raise KeyError('Requires SourceBucket, TargetBucket and Subfolder in events')\\n\\n    source_bucket = events['SourceBucket']\\n    target_bucket = events['TargetBucket']\\n    subfolder = events['Subfolder']\\n\\n    does_subfolder_exist_on_source = folder_exists_and_not_empty(source_bucket, subfolder)\\n    does_subfolder_exist_on_target = folder_exists_and_not_empty(target_bucket, subfolder)\\n\\n    return {'NoSubfolderOnBothBuckets': not does_subfolder_exist_on_source and not does_subfolder_exist_on_target}\\n\\ndef folder_exists_and_not_empty(bucket: str, path: str) -> bool:\\n    '''\\n    Folder should exists.\\n    Folder should not be empty.\\n    '''\\n    s3 = boto3.client('s3')\\n    if not path.endswith('/'):\\n        path = path + '/'\\n    resp = s3.list_objects_v2(Bucket=bucket, Prefix=path, Delimiter='/', MaxKeys=1)\\n    return 'Contents' in resp\"\n    }\n  }, {\n    \"name\" : \"AssertNoSubfolderOnBothBuckets\",\n    \"description\" : \"End the execution if the subfolder exists on one of the buckets.\",\n    \"action\" : \"aws:branch\",\n    \"inputs\" : {\n      \"Choices\" : [ {\n        \"NextStep\" : \"ApproveSubfolderCreation\",\n        \"Variable\" : \"{{ CheckNoSubfolderOnBothBuckets.NoSubfolderOnBothBuckets }}\",\n        \"BooleanEquals\" : true\n      } ]\n    },\n    \"isEnd\" : true\n  }, {\n    \"name\" : \"ApproveSubfolderCreation\",\n    \"description\" : \"Request an approval from the user to create the dedicated subfolder for the test.\",\n    \"action\" : \"aws:approve\",\n    \"timeoutSeconds\" : 3600,\n    \"onFailure\" : \"Abort\",\n    \"inputs\" : {\n      \"NotificationArn\" : \"{{SNSTopicARNForManualApproval}}\",\n      \"Message\" : \"A dedicated subfolder on the {{S3BucketWhereObjectsWillBeDeletedFrom}} will be created for the S3 accidental_delete test, which will be deleted at the end of the test. Do you approve?\",\n      \"MinRequiredApprovals\" : 1,\n      \"Approvers\" : [ \"{{IAMPrincipalForManualApproval}}\" ]\n    }\n  }, {\n    \"name\" : \"CreateSubfolderWithTestFiles\",\n    \"description\" : \"Create a dedicated subfolder with test files.\",\n    \"action\" : \"aws:executeScript\",\n    \"outputs\" : [ {\n      \"Name\" : \"Subfolder\",\n      \"Selector\" : \"$.Payload.Subfolder\",\n      \"Type\" : \"String\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"create_test_subfolder_with_test_objects\",\n      \"InputPayload\" : {\n        \"S3BucketName\" : \"{{S3BucketWhereObjectsWillBeDeletedFrom}}\",\n        \"Subfolder\" : \"{{GenerateSubfolderNameBeforeCreation.Subfolder}}\",\n        \"NumberOfObjectsToPut\" : 10\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\n\\ndef create_test_subfolder_with_test_objects(events, context):\\n    s3_client = boto3.client('s3')\\n\\n    if ('NumberOfObjectsToPut' not in events or 'S3BucketName' not in events\\n            or 'Subfolder' not in events):\\n        raise KeyError('Requires NumberOfObjectsToPut, S3BucketName and Subfolder in events')\\n\\n    bucket_name = events['S3BucketName']\\n    number_of_objects_to_put = events['NumberOfObjectsToPut']\\n    subfolder = events['Subfolder']\\n\\n    for i in range(int(number_of_objects_to_put)):\\n        s3_client.put_object(Bucket=bucket_name, Key=f'{subfolder}/{i}.txt',\\n                             Body=bytes(f'Content of the {i} file', encoding='utf-8'))\\n\\n    return {'Subfolder': subfolder}\"\n    }\n  }, {\n    \"name\" : \"AssertAlarmToBeGreenBeforeTest\",\n    \"description\" : \"Ensure alarm is green before starting test. Fail if alarm is not green within expected time.\",\n    \"action\" : \"aws:waitForAwsResourceProperty\",\n    \"inputs\" : {\n      \"Service\" : \"cloudwatch\",\n      \"Api\" : \"DescribeAlarms\",\n      \"AlarmNames\" : [ \"{{S3UserErrorAlarmName}}\" ],\n      \"PropertySelector\" : \"$.MetricAlarms[0].StateValue\",\n      \"DesiredValues\" : [ \"OK\" ]\n    }\n  }, {\n    \"name\" : \"BackupCurrentExecution\",\n    \"description\" : \"Copy the files from the subfolder on the source bucket to the subfolder on the backup bucket.\",\n    \"action\" : \"aws:executeScript\",\n    \"onFailure\" : \"step:DeleteSubfolderAfterTest\",\n    \"onCancel\" : \"step:DeleteSubfolderAfterTest\",\n    \"outputs\" : [ {\n      \"Name\" : \"CopiedFilesNumber\",\n      \"Selector\" : \"$.Payload.CopiedFilesNumber\",\n      \"Type\" : \"Integer\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"copy_to_subfolder\",\n      \"InputPayload\" : {\n        \"SourceBucket\" : \"{{S3BucketWhereObjectsWillBeDeletedFrom}}\",\n        \"TargetBucket\" : \"{{S3BucketToRestoreWhereObjectWillBeCopiedTo}}\",\n        \"Subfolder\" : \"{{CreateSubfolderWithTestFiles.Subfolder}}\"\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\n\\ndef copy_to_subfolder(events, context):\\n    if ('SourceBucket' not in events or 'TargetBucket' not in events\\n            or 'Subfolder' not in events):\\n        raise KeyError('Requires SourceBucket, TargetBucketName and Subfolder in events')\\n\\n    source_bucket = events[\\\"SourceBucket\\\"]\\n    target_bucket = events[\\\"TargetBucket\\\"]\\n    subfolder = events[\\\"Subfolder\\\"]\\n\\n    if not subfolder.endswith('/'):\\n        subfolder = subfolder + '/'\\n\\n    copied_count = copy_from_bucket_to_bucket(source_bucket, target_bucket, subfolder)\\n\\n    return {'CopiedFilesNumber': copied_count}\\n\\n\\n\\ndef copy_from_bucket_to_bucket(source_bucket, target_bucket, prefix=None):\\n    config = Config(retries={'max_attempts': 20, 'mode': 'standard'})\\n    s3_client = boto3.client('s3', config=config)\\n    paginator = s3_client.get_paginator('list_objects_v2')\\n    if prefix:\\n        pages = paginator.paginate(Bucket=source_bucket, Prefix=prefix)\\n        print(f'Starting to copy files under prefix {prefix} from the {source_bucket} bucket '\\n              f'to the {target_bucket} bucket...')\\n    else:\\n        pages = paginator.paginate(Bucket=source_bucket)\\n        print(f'Starting to copy files from the {source_bucket} bucket '\\n              f'to the {target_bucket} bucket...')\\n\\n    copied_count = 0\\n    for page in pages:\\n        print(f'The response from the list_objects_v2: {page}')\\n        if 'Contents' in page:\\n            for content in page[\\\"Contents\\\"]:\\n                print(f'Copying the file {content[\\\"Key\\\"]}...')\\n\\n                copy_source = {\\n                    'Bucket': source_bucket,\\n                    'Key': content[\\\"Key\\\"]\\n                }\\n                s3_client.copy(copy_source, target_bucket, content[\\\"Key\\\"])\\n\\n                print(f'The file {content[\\\"Key\\\"]} was successfully copied')\\n\\n                copied_count += 1\\n\\n    print(f'The file number of copied files is {copied_count}')\\n\\n    return copied_count\"\n    }\n  }, {\n    \"name\" : \"InjectFailure\",\n    \"description\" : \"If the dedicated subfolder exists on the source bucket, clean it.\",\n    \"action\" : \"aws:executeScript\",\n    \"onFailure\" : \"step:DeleteSubfolderAfterTest\",\n    \"onCancel\" : \"step:DeleteSubfolderAfterTest\",\n    \"outputs\" : [ {\n      \"Name\" : \"NumberOfDeletedObjects\",\n      \"Selector\" : \"$.Payload.NumberOfDeletedObjects\",\n      \"Type\" : \"Integer\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"clean_subfolder_if_exists\",\n      \"InputPayload\" : {\n        \"Bucket\" : \"{{S3BucketWhereObjectsWillBeDeletedFrom}}\",\n        \"Subfolder\" : \"{{CreateSubfolderWithTestFiles.Subfolder}}\"\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\n\\ndef clean_subfolder_if_exists(events, context):\\n    if 'Bucket' not in events or 'Subfolder' not in events:\\n        raise KeyError('Requires Bucket, Subfolder in events')\\n\\n    bucket = events[\\\"Bucket\\\"]\\n    subfolder = events[\\\"Subfolder\\\"]\\n\\n    does_subfolder_exist_on_target = folder_exists_and_not_empty(bucket, subfolder)\\n    number_of_deleted_objects = 0\\n\\n    if does_subfolder_exist_on_target:\\n        clean_bucket_output = clean_bucket({\\n            \\\"S3BucketNameToClean\\\": bucket,\\n            \\\"Prefix\\\": subfolder\\n        }, None)\\n        number_of_deleted_objects = clean_bucket_output['NumberOfDeletedObjects']\\n\\n    return {'NumberOfDeletedObjects': number_of_deleted_objects}\\n\\n\\n\\ndef folder_exists_and_not_empty(bucket: str, path: str) -> bool:\\n    '''\\n    Folder should exists.\\n    Folder should not be empty.\\n    '''\\n    s3 = boto3.client('s3')\\n    if not path.endswith('/'):\\n        path = path + '/'\\n    resp = s3.list_objects_v2(Bucket=bucket, Prefix=path, Delimiter='/', MaxKeys=1)\\n    return 'Contents' in resp\\n\\n\\n\\ndef clean_bucket(events, context):\\n    \\\"\\\"\\\"\\n    Clean bucket by removing versioned objects and delete markers\\n    :return: Number of removed versioned objects and delete markers\\n    \\\"\\\"\\\"\\n    if 'S3BucketNameToClean' not in events:\\n        raise KeyError('Requires S3BucketNameToClean in events')\\n\\n    s3_bucket_name_to_clean = events['S3BucketNameToClean']\\n    prefix = events.get('Prefix') or None\\n\\n    config = Config(retries={'max_attempts': 20, 'mode': 'standard'})\\n    s3_client = boto3.client('s3', config=config)\\n    paginator = s3_client.get_paginator('list_object_versions')\\n\\n    if prefix:\\n        pages = paginator.paginate(Bucket=s3_bucket_name_to_clean, Prefix=prefix)\\n        print(f'Sending the list_object_versions request for all objects under the {prefix} prefix,'\\n              f' of the {s3_bucket_name_to_clean} bucket...')\\n    else:\\n        pages = paginator.paginate(Bucket=s3_bucket_name_to_clean)\\n        print(f'Sending the list_object_versions request for all objects of the {s3_bucket_name_to_clean} bucket...')\\n\\n    number_of_deleted_objects = 0\\n\\n    for page in pages:\\n        print(f'The response from the list_object_versions: {page}')\\n\\n        versions: list = page.get('Versions')\\n        if versions is not None:\\n            for version in versions:\\n                key = version.get('Key')\\n                version_id = version.get('VersionId')\\n                s3_client.delete_object(Bucket=s3_bucket_name_to_clean, Key=key, VersionId=version_id)\\n\\n                print(f'The versioned object with Bucket={s3_bucket_name_to_clean}, '\\n                      f'Key={key}, VersionId={version_id} was deleted')\\n\\n                number_of_deleted_objects += 1\\n\\n        delete_markers: list = page.get('DeleteMarkers')\\n        if delete_markers is not None:\\n            for delete_marker in delete_markers:\\n                key = delete_marker.get('Key')\\n                version_id = delete_marker.get('VersionId')\\n                s3_client.delete_object(Bucket=s3_bucket_name_to_clean, Key=key, VersionId=version_id)\\n\\n                print(f'The delete marker with Bucket={s3_bucket_name_to_clean},'\\n                      f' Key={key}, VersionId={version_id} was deleted')\\n\\n                number_of_deleted_objects += 1\\n\\n    print(f'The number of deleted versioned objects and delete markers '\\n          f'in restore bucket is {number_of_deleted_objects}')\\n\\n    return {'NumberOfDeletedObjects': number_of_deleted_objects}\"\n    }\n  }, {\n    \"name\" : \"AssertAlarmToBeRed\",\n    \"description\" : \"Wait for expected alarm to be red after failure is injected\",\n    \"action\" : \"aws:waitForAwsResourceProperty\",\n    \"onFailure\" : \"step:DeleteSubfolderAfterTest\",\n    \"onCancel\" : \"step:DeleteSubfolderAfterTest\",\n    \"maxAttempts\" : 1,\n    \"timeoutSeconds\" : 600,\n    \"inputs\" : {\n      \"Service\" : \"cloudwatch\",\n      \"Api\" : \"DescribeAlarms\",\n      \"AlarmNames\" : [ \"{{S3UserErrorAlarmName}}\" ],\n      \"PropertySelector\" : \"$.MetricAlarms[0].StateValue\",\n      \"DesiredValues\" : [ \"ALARM\" ]\n    }\n  }, {\n    \"name\" : \"RollbackCurrentExecution\",\n    \"description\" : \"Copy the files from the subfolder on the backup bucket to the subfolder on the source bucket.\",\n    \"action\" : \"aws:executeScript\",\n    \"onFailure\" : \"step:DeleteSubfolderAfterTest\",\n    \"onCancel\" : \"step:DeleteSubfolderAfterTest\",\n    \"outputs\" : [ {\n      \"Name\" : \"CopiedFilesNumber\",\n      \"Selector\" : \"$.Payload.CopiedFilesNumber\",\n      \"Type\" : \"Integer\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"copy_to_subfolder\",\n      \"InputPayload\" : {\n        \"SourceBucket\" : \"{{S3BucketToRestoreWhereObjectWillBeCopiedTo}}\",\n        \"TargetBucket\" : \"{{S3BucketWhereObjectsWillBeDeletedFrom}}\",\n        \"Subfolder\" : \"{{CreateSubfolderWithTestFiles.Subfolder}}\"\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\n\\ndef copy_to_subfolder(events, context):\\n    if ('SourceBucket' not in events or 'TargetBucket' not in events\\n            or 'Subfolder' not in events):\\n        raise KeyError('Requires SourceBucket, TargetBucketName and Subfolder in events')\\n\\n    source_bucket = events[\\\"SourceBucket\\\"]\\n    target_bucket = events[\\\"TargetBucket\\\"]\\n    subfolder = events[\\\"Subfolder\\\"]\\n\\n    if not subfolder.endswith('/'):\\n        subfolder = subfolder + '/'\\n\\n    copied_count = copy_from_bucket_to_bucket(source_bucket, target_bucket, subfolder)\\n\\n    return {'CopiedFilesNumber': copied_count}\\n\\n\\n\\ndef copy_from_bucket_to_bucket(source_bucket, target_bucket, prefix=None):\\n    config = Config(retries={'max_attempts': 20, 'mode': 'standard'})\\n    s3_client = boto3.client('s3', config=config)\\n    paginator = s3_client.get_paginator('list_objects_v2')\\n    if prefix:\\n        pages = paginator.paginate(Bucket=source_bucket, Prefix=prefix)\\n        print(f'Starting to copy files under prefix {prefix} from the {source_bucket} bucket '\\n              f'to the {target_bucket} bucket...')\\n    else:\\n        pages = paginator.paginate(Bucket=source_bucket)\\n        print(f'Starting to copy files from the {source_bucket} bucket '\\n              f'to the {target_bucket} bucket...')\\n\\n    copied_count = 0\\n    for page in pages:\\n        print(f'The response from the list_objects_v2: {page}')\\n        if 'Contents' in page:\\n            for content in page[\\\"Contents\\\"]:\\n                print(f'Copying the file {content[\\\"Key\\\"]}...')\\n\\n                copy_source = {\\n                    'Bucket': source_bucket,\\n                    'Key': content[\\\"Key\\\"]\\n                }\\n                s3_client.copy(copy_source, target_bucket, content[\\\"Key\\\"])\\n\\n                print(f'The file {content[\\\"Key\\\"]} was successfully copied')\\n\\n                copied_count += 1\\n\\n    print(f'The file number of copied files is {copied_count}')\\n\\n    return copied_count\"\n    }\n  }, {\n    \"name\" : \"AssertAlarmToBeGreen\",\n    \"description\" : \"Wait for the alarm to be green after test is complete\",\n    \"action\" : \"aws:waitForAwsResourceProperty\",\n    \"onFailure\" : \"step:DeleteSubfolderAfterTest\",\n    \"onCancel\" : \"step:DeleteSubfolderAfterTest\",\n    \"maxAttempts\" : 1,\n    \"timeoutSeconds\" : 1200,\n    \"inputs\" : {\n      \"Service\" : \"cloudwatch\",\n      \"Api\" : \"DescribeAlarms\",\n      \"AlarmNames\" : [ \"{{S3UserErrorAlarmName}}\" ],\n      \"PropertySelector\" : \"$.MetricAlarms[0].StateValue\",\n      \"DesiredValues\" : [ \"OK\" ]\n    }\n  }, {\n    \"name\" : \"DeleteSubfolderAfterTest\",\n    \"description\" : \"If the dedicated subfolder exists on the source bucket, clean it.\",\n    \"action\" : \"aws:executeScript\",\n    \"outputs\" : [ {\n      \"Name\" : \"NumberOfDeletedObjects\",\n      \"Selector\" : \"$.Payload.NumberOfDeletedObjects\",\n      \"Type\" : \"Integer\"\n    } ],\n    \"inputs\" : {\n      \"Runtime\" : \"python3.8\",\n      \"Handler\" : \"clean_subfolder_if_exists\",\n      \"InputPayload\" : {\n        \"Bucket\" : \"{{S3BucketWhereObjectsWillBeDeletedFrom}}\",\n        \"Subfolder\" : \"{{CreateSubfolderWithTestFiles.Subfolder}}\"\n      },\n      \"Script\" : \"from datetime import datetime, timezone\\nimport math\\nimport boto3\\nfrom botocore.config import Config\\nimport time\\nimport dateutil.parser\\n\\n\\n\\n\\ndef clean_subfolder_if_exists(events, context):\\n    if 'Bucket' not in events or 'Subfolder' not in events:\\n        raise KeyError('Requires Bucket, Subfolder in events')\\n\\n    bucket = events[\\\"Bucket\\\"]\\n    subfolder = events[\\\"Subfolder\\\"]\\n\\n    does_subfolder_exist_on_target = folder_exists_and_not_empty(bucket, subfolder)\\n    number_of_deleted_objects = 0\\n\\n    if does_subfolder_exist_on_target:\\n        clean_bucket_output = clean_bucket({\\n            \\\"S3BucketNameToClean\\\": bucket,\\n            \\\"Prefix\\\": subfolder\\n        }, None)\\n        number_of_deleted_objects = clean_bucket_output['NumberOfDeletedObjects']\\n\\n    return {'NumberOfDeletedObjects': number_of_deleted_objects}\\n\\n\\n\\ndef folder_exists_and_not_empty(bucket: str, path: str) -> bool:\\n    '''\\n    Folder should exists.\\n    Folder should not be empty.\\n    '''\\n    s3 = boto3.client('s3')\\n    if not path.endswith('/'):\\n        path = path + '/'\\n    resp = s3.list_objects_v2(Bucket=bucket, Prefix=path, Delimiter='/', MaxKeys=1)\\n    return 'Contents' in resp\\n\\n\\n\\ndef clean_bucket(events, context):\\n    \\\"\\\"\\\"\\n    Clean bucket by removing versioned objects and delete markers\\n    :return: Number of removed versioned objects and delete markers\\n    \\\"\\\"\\\"\\n    if 'S3BucketNameToClean' not in events:\\n        raise KeyError('Requires S3BucketNameToClean in events')\\n\\n    s3_bucket_name_to_clean = events['S3BucketNameToClean']\\n    prefix = events.get('Prefix') or None\\n\\n    config = Config(retries={'max_attempts': 20, 'mode': 'standard'})\\n    s3_client = boto3.client('s3', config=config)\\n    paginator = s3_client.get_paginator('list_object_versions')\\n\\n    if prefix:\\n        pages = paginator.paginate(Bucket=s3_bucket_name_to_clean, Prefix=prefix)\\n        print(f'Sending the list_object_versions request for all objects under the {prefix} prefix,'\\n              f' of the {s3_bucket_name_to_clean} bucket...')\\n    else:\\n        pages = paginator.paginate(Bucket=s3_bucket_name_to_clean)\\n        print(f'Sending the list_object_versions request for all objects of the {s3_bucket_name_to_clean} bucket...')\\n\\n    number_of_deleted_objects = 0\\n\\n    for page in pages:\\n        print(f'The response from the list_object_versions: {page}')\\n\\n        versions: list = page.get('Versions')\\n        if versions is not None:\\n            for version in versions:\\n                key = version.get('Key')\\n                version_id = version.get('VersionId')\\n                s3_client.delete_object(Bucket=s3_bucket_name_to_clean, Key=key, VersionId=version_id)\\n\\n                print(f'The versioned object with Bucket={s3_bucket_name_to_clean}, '\\n                      f'Key={key}, VersionId={version_id} was deleted')\\n\\n                number_of_deleted_objects += 1\\n\\n        delete_markers: list = page.get('DeleteMarkers')\\n        if delete_markers is not None:\\n            for delete_marker in delete_markers:\\n                key = delete_marker.get('Key')\\n                version_id = delete_marker.get('VersionId')\\n                s3_client.delete_object(Bucket=s3_bucket_name_to_clean, Key=key, VersionId=version_id)\\n\\n                print(f'The delete marker with Bucket={s3_bucket_name_to_clean},'\\n                      f' Key={key}, VersionId={version_id} was deleted')\\n\\n                number_of_deleted_objects += 1\\n\\n    print(f'The number of deleted versioned objects and delete markers '\\n          f'in restore bucket is {number_of_deleted_objects}')\\n\\n    return {'NumberOfDeletedObjects': number_of_deleted_objects}\"\n    },\n    \"isEnd\" : true\n  } ]\n}",
  "CreatedDate": "2023-02-19T17:15:43.132Z",
  "DisplayName": null,
  "DocumentFormat": {
    "Value": "JSON"
  },
  "DocumentType": {
    "Value": "Automation"
  },
  "DocumentVersion": "4",
  "Name": "AWSResilienceHub-SimulateS3ObjectsAccidentalDeleteTest_2020-04-01",
  "Requires": [],
  "ReviewStatus": null,
  "Status": {
    "Value": "Active"
  },
  "StatusInformation": null,
  "VersionName": null,
  "ResponseMetadata": {
    "RequestId": "88e979ae-c0a3-4bc4-8851-ce399bcb02c2",
    "Metadata": {},
    "ChecksumAlgorithm": 0,
    "ChecksumValidationStatus": 0
  },
  "ContentLength": 28086,
  "HttpStatusCode": 200,
  "LoggedAt": "2023-03-15T07:12:21.2520369+00:00"
}
